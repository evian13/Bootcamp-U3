{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRILL:\n",
    "Let's say we work at a credit card company and we're trying to figure out if people are going to pay their bills on time. We have everyone's purchases, split into four main categories: groceries, dining out, utilities, and entertainment. What are some ways you might use KNN to create this model? What aspects of KNN would be useful? Write up your thoughts in submit a link below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a KNN classifier.\n",
    "\n",
    "As a first note, the probability to pay the bills should be higher for people who are spending more money on dinning out and entertinment. The logic here is the more you spend on dinning out, the more money you have at your disposal. \n",
    "\n",
    "We could plot a boxplot for each feature to see how the data is spreaded around the median and how many ouliers are. \n",
    "\n",
    "But having more money does not direclty imply that they will pay on time. We could analyze the \"utilities\" column to see if they pay \"on time\". Depending on the country we scrutinize we should understand how regularly the bills are issued by each supplier: monthly, quarterly etc. Using this information we could create a new binary feature: who is late on payments and who is paying on time. Another helpful binary feature could be if they are using the \"direct debit\" option.\n",
    "\n",
    "\"groceries\" and \"dining out\" should be in opposition: the more they eat out the less they spend on cooking home. We cannot say precisely that those who eat at home are less inclined to pay the bills on time, but connected with the next feature would shed more light upon the conclusion.\n",
    "\n",
    "Another feature that could help is \"education\": usually, the more educated you are the more social responsable you become; so paying bills on time will be a habit.\n",
    "\n",
    "><i>What aspects of KNN would be useful? </i>\n",
    "\n",
    "For each column we have to see if it is normally distributed. If not we could apply transformations: log, square root, square etc.\n",
    "\n",
    "The normality helps in applying the z-score to rescale the features that are big numbers. The Euclidean distance would otherwise be skewed.\n",
    "\n",
    "Afterwards we'll try to find the right \"k\". If the \"k\" is too small the model will pick up more subtle deviations, but these deviations could be just randomness and therefore you could just be overfitting. We could use the \"weighting\" procedure to help those who are closer to have more power in the final result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
